---
layout: post

title: 集群证书升级

categories: [云]

---



## 集群证书升级

问题现象：kubectl指令执行报错**证书过期**的时候执行了`kubeadm certs renew all`,后续kubectl执行正常能顺利查看pod信息。但是部署应用报错，这里是apiserver中的报错日志：

![image-20251226145958728](https://yimacro.github.io/pics/post/image-20251226145958728.png)

*这下面是针对 Kubernetes 集群证书过期的详细操作手册。当证书过期时，Master 节点上的 `kubectl` 指令将无法正常工作，并报错提示证书失效*

*请按照以下步骤，在相应的机器上执行操作。*



第一阶段：在 Master 节点上执行（控制平面）

如果您的集群有多个 Master 节点（高可用架构），**必须登录到每一台 Master 节点**，逐一重复以下所有步骤

1. 基础准备与备份

• **查看过期时间**： `sudo kubeadm certs check-expiration` *说明：此命令用于确认哪些证书已过期以及具体的到期日期*

• **备份现有证书**（以防操作失误）： 

`cp -r /etc/kubernetes/ /tmp/backup/` 

`cp -r /var/lib/kubelet/pki/ /tmp/backup.crr` 

*说明：在进行重大变更前，备份* */etc/kubernetes* *目录是标准的安全操作*

2. 执行证书续期

• **执行指令**： `sudo kubeadm certs renew all` *说明：该命令会*更新 Kubernetes 内部所有的核心证书*。注意，它主要更新证书文件本身，但可能不会自动更新某些配置文件（.conf）中的参数*

3. 重启核心组件（关键步骤）

**注意：** 仅更新文件是不够的，API Server 等组件在内存中仍运行着旧证书。必须通过以下“移动文件法”强制它们重启

• **移动配置文件以停止容器**：

 `mkdir -p /tmp/k8s-renew`

 `mv /etc/kubernetes/manifests/*.yaml /tmp/k8s-renew/` 

*说明：Kubernetes 会监控* */etc/kubernetes/manifests/* *目录。将 YAML 文件移走后，系统会自动停止对应的静态 Pod（API Server, Scheduler, Controller Manager）*

• **等待并确认**： 等待 20-30 秒，执行 

`crictl -r unix:///var/run/containerd/containerd.sock ps`

 *说明：确认上述核心组件的容器已经消失*

• **移回配置文件以重启容器**： `sudo mv /tmp/k8s-renew/*.yaml /etc/kubernetes/manifests/` *说明：文件移回后，Kubernetes 会感知到新文件并***使用新证书重新启动这些核心组件

4. 更新管理权限（Kubeconfig）

• **更新管理员配置**：

`cp /root/.kube/config /tmp/kube.old/config`

`cp /etc/kubernetes/admin.conf ~/.kube/config`

*说明：*admin.conf *包含了* *kubectl* *与集群通信所需的凭据。如果不更新此文件，你在 Master 上依然无法使用* *kubectl* *指令*



5. 重启 Kubelet 服务

• **执行指令**： 

`sudo systemctl restart kubelet` 

*说明：确保该节点的工作代理进程也加载并识别最新的证书*

---



第二阶段：在 Worker 节点上执行（工作节点）

通常情况下，Worker 节点支持证书自动轮换，操作较为简单

• **常规重启**： `sudo systemctl restart kubelet` *说明：在***每一台 Worker 节点***上执行此命令。大多数情况下，这足以让节点恢复正常连接*

---



第三阶段：特殊故障处理（当 Worker 节点报错 x509 证书错误）

如果重启 Kubelet 后，Worker 节点依然无法加入集群（报错证书认证失败），请执行以下强制重入步骤：

1. **在 Master 节点上**生成新的加入命令： `kubeadm token create --print-join-command` *说明：这会生成一条包含 Token 和哈希值的完整* *kubeadm join* *指令*

2. **在出问题的 Worker 节点上**重置并重新加入：

  ◦ **重置节点**：`sudo kubeadm reset -f` *注意：这会清除该节点当前的集群配置*

  ◦ **执行加入命令**：复制第 1 步中 Master 生成的那行以 `kubeadm join` 开头的指令并运行






